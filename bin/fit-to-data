#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
from datetime import datetime

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import matplotlib.pyplot as plt
import numpy as np
import optax
import yaml
from flowjax.distributions import Normal
from flowjax.bijections import RationalQuadraticSpline
from scipy.stats import chi2

from llhdflow import Histogram, chi2_analysis, fit, masked_autoregressive_flow


def plot_history(losses, name):
    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))
    ax0.plot(losses["train"], label="train")
    ax0.plot(losses["val"], label="val")
    ax0.legend()
    if np.all(np.greater_equal(losses["train"], 0.0)) and np.all(
        np.greater_equal(losses["train"], 0.0)
    ):
        ax0.set_yscale("log")
    ax1.plot(losses["lr"])
    ax1.set_yscale("log")
    plt.savefig(name)
    plt.close("all")


def main(args):
    jax.config.update("jax_platform_name", "gpu")

    data = np.load(args.DATAPATH)
    X_train = data["X_train"]
    dim = X_train.shape[1]
    bins = chi2.ppf(np.arange(0.0, 1.1, 0.1), df=dim)

    transformer = {
        "affine": None,
        "rqs": RationalQuadraticSpline(knots=args.KNOTS, interval=args.INTERVAL),
    }[args.TRANS]

    while True:

        # check if there are any completed scans
        # important in case of a parallel scans running at the same time
        if any("-COMPLETE" in fl for fl in os.listdir(os.path.split(args.OUTPATH)[0])):
            print("Found a completed scan.")
            break

        key, subkey = jr.split(jr.key(np.random.randint(0, high=999999999999)))
        flow = masked_autoregressive_flow(
            subkey,
            base_dist=Normal(jnp.zeros(dim)),
            transformer=transformer,
            nn_width=args.NNWIDTH,
            nn_depth=args.NNDEPTH,
            flow_layers=args.FLOWLAYERS,
            invert=True,
            nn_activation=jax.nn.relu,
            permutation=list(reversed(range(dim))),
        )

        optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=args.LR)
        scheduler = optax.exponential_decay(
            init_value=args.LR,
            transition_steps=25,
            decay_rate=0.5,
            staircase=True,
            end_value=args.MINLR,
        )

        flow, losses = fit(
            key,
            flow,
            X_train,
            optimizer=optimizer,
            max_epochs=args.EPOCHS,
            max_patience=50,
            lr_scheduler=scheduler,
        )

        gauss_test = np.array(jax.vmap(flow.bijection.inverse, in_axes=0)(data["X_test"]))

        hist = Histogram(
            dim=dim, bins=bins, max_val=20, vals=np.sum(gauss_test**2, axis=1)
        )

        chi2_analysis(
            gauss_test,
            bins=np.hstack([chi2.ppf(np.arange(0.0, 1, 0.1), df=dim), [20.0]]),
            plot_name=os.path.join(args.OUTPATH, "chi2.png"),
        )
        eqx.tree_serialise_leaves(os.path.join(args.OUTPATH, "model.eqx"), flow)
        plot_history(losses, os.path.join(args.OUTPATH, "history.png"))

        pull = hist.pull
        pval = 1.0 - chi2.cdf(np.sum(pull**2), df=len(pull))
        print(
            f"1-CDF(residuals) = {pval*100:.2f}%, Residuals per bin:"
            + ", ".join(map(lambda x: f"{x:.2e}", pull))
        )
        if pval >= 0.03:
            np.savez_compressed(
                os.path.join(args.OUTPATH, "deviations.npz"), deviations=gauss_test
            )
            model_config = {
                "flow_layers": args.FLOWLAYERS,
                "nn_depth": args.NNDEPTH,
                "nn_width": args.NNWIDTH,
                "dim": dim,
                "activation": "relu",
                "transformer": args.TRANS,
            }
            if args.TRANS == "rsq":
                model_config.update(
                    {
                        "transformer_kwargs": {
                            "knots": args.KNOTS,
                            "interval": args.INTERVAL,
                        }
                    }
                )
            with open(
                os.path.join(args.OUTPATH, "model_config.yaml"), "w", encoding="utf-8"
            ) as f:
                yaml.safe_dump(model_config, f)
            os.rename(args.OUTPATH, args.OUTPATH + "-COMPLETE")
            break


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Search model space for proper nflow")

    parameters = parser.add_argument_group("Hyperparameters")
    parameters.add_argument(
        "--nn-width",
        "-w",
        type=int,
        default=32,
        help="Number of nodes per layer",
        dest="NNWIDTH",
    )
    parameters.add_argument(
        "--nn-depth",
        "-d",
        type=int,
        default=3,
        help="Number of layers",
        dest="NNDEPTH",
    )
    parameters.add_argument(
        "--flow-layers",
        "-l",
        type=int,
        default=2,
        help="Number of flow layers",
        dest="FLOWLAYERS",
    )
    parameters.add_argument(
        "-lr", type=float, default=1e-3, help="learning rate", dest="LR"
    )
    parameters.add_argument(
        "-mlr", type=float, default=1e-4, help="minlearning rate", dest="MINLR"
    )
    parameters.add_argument(
        "--epochs", "-e", type=int, default=600, help="Number of epochs", dest="EPOCHS"
    )

    ansatz = parser.add_argument_group("Ansatz")
    ansatz.add_argument(
        "--transformer",
        "-t",
        type=str,
        default="affine",
        choices=["affine", "rqs"],
        help="Choices of transformers to be used in MAF, defaults to affine.",
        dest="TRANS",
    )
    ansatz.add_argument(
        "--knots",
        "-k",
        type=int,
        default=6,
        help="Knots for RQS only, defaults to 6",
        dest="KNOTS",
    )
    ansatz.add_argument(
        "--interval",
        "-int",
        type=int,
        default=4,
        help="Interval for RQS only, defaults to 4",
        dest="INTERVAL",
    )

    dataset = parser.add_argument_group("Dataset")
    dataset.add_argument(
        "--data-path",
        "-dp",
        type=str,
        help="NumPy file includes standardised dataset. File should include 'X_train` and `X_test` keyword arguments.",
        dest="DATAPATH",
    )

    paths = parser.add_argument_group("Paths")
    paths.add_argument(
        "--out-path",
        "-op",
        type=str,
        help="Output path, defaults to " + os.path.join(os.getcwd(), "results"),
        dest="OUTPATH",
        default=os.path.join(os.getcwd(), "results"),
    )
    paths.add_argument(
        "--out-name",
        "-on",
        type=str,
        help="Output name, default to " + datetime.now().strftime("%b%d-%H-%M-%S"),
        dest="OUTNAME",
        default=datetime.now().strftime("%b%d-%H-%M-%S"),
    )

    args = parser.parse_args()

    if not os.path.isdir(args.OUTPATH):
        os.mkdir(args.OUTPATH)

    args.OUTPATH = os.path.join(args.OUTPATH, args.OUTNAME)
    if not os.path.isdir(args.OUTPATH):
        os.mkdir(args.OUTPATH)

    print("<><><> Arguments <><><>")
    for key, item in vars(args).items():
        print(f"   * {key} : {item}")
    print("<><><><><><><><><><><>")

    arg_dict = vars(args)
    arg_dict.update({"HOSTNAME": os.environ.get("HOSTNAME", None)})
    with open(os.path.join(args.OUTPATH, "config.yaml"), "w") as f:
        yaml.safe_dump(arg_dict, f)

    main(args)
