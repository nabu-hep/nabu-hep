#!/usr/bin/env python3
# pylint: disable=no-member,ungrouped-imports,invalid-name

import argparse
import os
from datetime import datetime

import jax
import numpy as np
import optax
import yaml
from scipy.stats import chi2

from nabu import Likelihood
from nabu.flow import RationalQuadraticSpline, available_activations, get_flow

try:
    import matplotlib.pyplot as plt

    from nabu.plotting import summary_plot

    def plot_history(losses: dict[str, list], name: str) -> None:
        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))
        ax0.plot(losses["train"], label="train")
        ax0.plot(losses["val"], label="val")
        ax0.legend()
        if np.all(np.greater_equal(losses["train"], 0.0)) and np.all(
            np.greater_equal(losses["train"], 0.0)
        ):
            ax0.set_yscale("log")
        ax1.plot(losses["lr"])
        ax1.set_yscale("log")
        plt.savefig(name)
        plt.close("all")

except NotImplementedError:
    summary_plot = None
    plot_history = None
    print("  -> Warning: Plotting is not available")


def main(args):
    data = np.load(args.DATAPATH)
    X_train = data["X_train"]
    dim = X_train.shape[1]
    if 0.0 < args.DATAFRAC < 1.0:
        X_train = X_train[: int(X_train.shape[0] * args.DATAFRAC)]

    transformer = {
        "affine": None,
        "rqs": RationalQuadraticSpline(knots=args.KNOTS, interval=args.INTERVAL),
    }[args.TRANS]

    best_pval = 0
    while True:
        # check if there are any completed scans
        # important in case of a parallel scans running at the same time
        if any("-COMPLETE" in fl for fl in os.listdir(os.path.split(args.OUTPATH)[0])):
            print("Found a completed scan.")
            break

        llhd_constructor = get_flow("masked_autoregressive_flow")
        likelihood: Likelihood = llhd_constructor(
            dim=dim,
            transformer=transformer,
            flow_layers=args.FLOWLAYERS,
            nn_width=args.NNWIDTH,
            nn_depth=args.NNDEPTH,
            activation=args.ACTIVATION,
            permutation=list(reversed(range(dim))),
            random_seed=42 if args.TEST else np.random.randint(0, high=999999999999),
        )

        scheduler = optax.exponential_decay(
            init_value=args.LR,
            transition_steps=args.TRANSSTEP,
            decay_rate=args.DECAYRATE,
            staircase=True,
            end_value=args.MINLR,
        )

        history = likelihood.fit_to_data(
            dataset=X_train,
            learning_rate=args.LR,
            optimizer="adam",
            lr_scheduler=scheduler,
            max_epochs=args.EPOCHS,
            max_patience=args.MAXPATIENCE,
            random_seed=42 if args.TEST else np.random.randint(0, high=999999999999),
        )

        hist = likelihood.goodness_of_fit(
            test_dataset=data["X_test"], prob_per_bin=args.PROB
        )
        pval = hist.residuals_pvalue
        kst_pval = hist.kstest_pval
        goodness_of_fit_test = all(np.greater_equal([pval, kst_pval], args.PVALTHRES))

        print(f"1-CDF(residuals) = {pval*100:.2f}%, KS p-val: {kst_pval*100:.2f}%")

        # save only the best pval results
        if pval > best_pval or goodness_of_fit_test:
            best_pval = pval
            if summary_plot is not None:
                _ = summary_plot(
                    likelihood=likelihood,
                    test_data=data["X_test"],
                    bins=np.hstack([chi2.ppf(np.arange(0.0, 1.0, 0.1), df=dim), [20.0]]),
                )
                plt.savefig(os.path.join(args.OUTPATH, "summary_plot"))
                plot_history(history, os.path.join(args.OUTPATH, "history.png"))
            likelihood.save(os.path.join(args.OUTPATH, "model.nabu"))

        if goodness_of_fit_test or args.TEST:
            np.savez_compressed(
                os.path.join(args.OUTPATH, "deviations.npz"),
                deviations=likelihood.compute_inverse(data["X_test"]),
            )
            os.rename(args.OUTPATH, args.OUTPATH + "-COMPLETE")
            break


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Search model space for proper nflow")

    parameters = parser.add_argument_group("Hyperparameters")
    parameters.add_argument(
        "--nn-width",
        "-w",
        type=int,
        default=32,
        help="Number of nodes per layer",
        dest="NNWIDTH",
    )
    parameters.add_argument(
        "--nn-depth",
        "-d",
        type=int,
        default=3,
        help="Number of layers",
        dest="NNDEPTH",
    )
    parameters.add_argument(
        "--flow-layers",
        "-l",
        type=int,
        default=2,
        help="Number of flow layers",
        dest="FLOWLAYERS",
    )
    parameters.add_argument(
        "--activation",
        "-a",
        type=str,
        default="relu",
        choices=available_activations(),
        help="Activation function for MLP.",
        dest="ACTIVATION",
    )
    parameters.add_argument(
        "-lr", type=float, default=1e-3, help="learning rate", dest="LR"
    )
    parameters.add_argument(
        "-mlr", type=float, default=1e-4, help="minlearning rate", dest="MINLR"
    )
    parameters.add_argument(
        "--max-patience",
        "-mp",
        type=int,
        default=50,
        help="Number of epochs for training to monitor validation "
        "loss before terminating. Defaults to 50.",
        dest="MAXPATIENCE",
    )
    parameters.add_argument(
        "--decay-rate",
        "-dr",
        type=float,
        default=0.5,
        help="Decay rate for LR scheduler, defaults to 0.5",
        dest="DECAYRATE",
    )
    parameters.add_argument(
        "--transition-steps",
        "-ts",
        type=int,
        default=25,
        help="Number of epochs for LR scheduler to wait before decaying, "
        "defaults to 25",
        dest="TRANSSTEP",
    )
    parameters.add_argument(
        "--epochs", "-e", type=int, default=600, help="Number of epochs", dest="EPOCHS"
    )
    parameters.add_argument(
        "--pval-threshold",
        "-pt",
        type=float,
        default=0.03,
        help="Threshold for 1-CDF of the residuals, defaults to >= 0.03",
        dest="PVALTHRES",
    )
    parameters.add_argument(
        "--prob-per-bin",
        "-p",
        type=float,
        default=0.1,
        help="Yield probability per bin for goodness of fit test, defaults to 0.1",
        dest="PROB",
    )

    ansatz = parser.add_argument_group("Ansatz")
    ansatz.add_argument(
        "--transformer",
        "-t",
        type=str,
        default="affine",
        choices=["affine", "rqs"],
        help="Choices of transformers to be used in MAF, defaults to affine.",
        dest="TRANS",
    )
    ansatz.add_argument(
        "--knots",
        "-k",
        type=int,
        default=6,
        help="Knots for RQS only, defaults to 6",
        dest="KNOTS",
    )
    ansatz.add_argument(
        "--interval",
        "-int",
        type=int,
        default=4,
        help="Interval for RQS only, defaults to 4",
        dest="INTERVAL",
    )

    dataset = parser.add_argument_group("Dataset")
    dataset.add_argument(
        "--data-path",
        "-dp",
        type=str,
        help="NumPy file includes standardised dataset. "
        + "File should include 'X_train` and `X_test` keyword arguments.",
        dest="DATAPATH",
    )
    dataset.add_argument(
        "--train-frac",
        "-tfrac",
        type=float,
        default=1.0,
        help="Fraction of the data to be used as the training set.",
        dest="DATAFRAC",
    )

    paths = parser.add_argument_group("Paths")
    paths.add_argument(
        "--out-path",
        "-op",
        type=str,
        help="Output path, defaults to " + os.path.join(os.getcwd(), "results"),
        dest="OUTPATH",
        default=os.path.join(os.getcwd(), "results"),
    )
    paths.add_argument(
        "--out-name",
        "-on",
        type=str,
        help="Output name, default to " + datetime.now().strftime("%b%d-%H-%M-%S"),
        dest="OUTNAME",
        default=datetime.now().strftime("%b%d-%H-%M-%S"),
    )

    execution = parser.add_argument_group("Execution")
    execution.add_argument(
        "-gpu",
        action="store_true",
        default=False,
        help="Run on GPU, defaults to False",
        dest="GPU",
    )
    execution.add_argument(
        "-test",
        action="store_true",
        default=False,
        help="Run on the Test routine. This routine fixes the random seeds and executes the algorithm once.",
        dest="TEST",
    )

    args = parser.parse_args()

    if not os.path.isdir(args.OUTPATH):
        os.mkdir(args.OUTPATH)

    args.OUTNAME = "TEST-RESULT" if args.TEST else args.OUTNAME

    args.OUTPATH = os.path.join(args.OUTPATH, args.OUTNAME)
    if not os.path.isdir(args.OUTPATH):
        os.mkdir(args.OUTPATH)

    print("<><><> Arguments <><><>")
    for key, item in vars(args).items():
        print(f"   * {key} : {item}")
    print("<><><><><><><><><><><>")

    arg_dict = vars(args)
    arg_dict.update({"HOSTNAME": os.environ.get("HOSTNAME", None)})
    with open(os.path.join(args.OUTPATH, "config.yaml"), "w", encoding="utf-8") as f:
        yaml.safe_dump(arg_dict, f)

    jax.config.update("jax_platform_name", "gpu" if args.GPU else "cpu")
    main(args)
